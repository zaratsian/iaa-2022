{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spark_ML_Assignment_(template).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWE-YUBsNthe"
      },
      "source": [
        "## **Spark ML Assignment**\n",
        "The goal of this assignment is to train a Spark ML Model. \n",
        "\n",
        "You may do this in any way that you'd like, but I've provided this template as a helpful framework, which you may find helpful. \n",
        "\n",
        "Assignment Tasks:\n",
        "\n",
        "1) Load the CSV into a Spark Dataframe. I've added code to automatically download \"banking_attrition.csv\", however, if you have another dataset you'd like to analyze, feel free to use that instead. \n",
        "\n",
        "2) Preprocess the data, applying any data exploration or cleaning techniques. \n",
        "\n",
        "3) Split the model into train and test\n",
        "\n",
        "4) Train the model (Please predict \"attrition\" probability if you use the banking_attrition.csv\" file). \n",
        "\n",
        "5) Apply the model again your test dataframe\n",
        "\n",
        "6) Display model evaluation \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBqp2EKl2mhX"
      },
      "source": [
        "## **Install PySpark Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXWkpWwR2gVR"
      },
      "source": [
        "# Install Spark dependencies\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!rm spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget --no-cookies --no-check-certificate https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar zxvf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEciKx-M2y75"
      },
      "source": [
        "## **Load Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYrbIewX23lA"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/zaratsian/Datasets/master/banking_attrition.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BpC4MLS25kj"
      },
      "source": [
        "## **Import Python / Spark Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lRZl0-Xt2_NE"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"]  = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "import datetime, time\n",
        "import re, random, sys\n",
        "\n",
        "# Note - Not all of these will be used, but I've added them for your reference as a \"getting started\"\n",
        "from pyspark import SparkContext\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField, ArrayType, IntegerType, StringType, FloatType, LongType, DateType\n",
        "from pyspark.sql.functions import struct, array, lit, monotonically_increasing_id, col, expr, when, concat, udf, split, size, lag, count, isnull\n",
        "from pyspark.sql import Window\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.regression import GBTRegressor, LinearRegression, GeneralizedLinearRegression, RandomForestRegressor\n",
        "from pyspark.ml.classification import GBTClassifier, RandomForestClassifier\n",
        "from pyspark.ml.feature import VectorIndexer, VectorAssembler, StringIndexer, IndexToString\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml import Pipeline, PipelineModel\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, RegressionEvaluator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvK0YRFx78RY"
      },
      "source": [
        "## **Create Spark Session**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwGkL0GW7-_j"
      },
      "source": [
        "spark = SparkSession.builder.appName(\"Spark ML Assignment\").master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnbtnMcp4v3E"
      },
      "source": [
        "## **Load CSV Data into Spark Dataframe**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "062IYTp24zQh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu8K-qCi2-Kk"
      },
      "source": [
        "## **Data Exploration**\n",
        "Perform at least one data exploration of your choice (This could be a basic show(), an aggregation/[groupby](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.groupBy), [correlation](https://spark.apache.org/docs/latest/ml-statistics.html#correlation), [summarizer](https://spark.apache.org/docs/latest/ml-statistics.html#summarizer), etc.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sBDC5db4FL-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkPpObFZ4Pax"
      },
      "source": [
        "## **Split the Spark Dataframe into Train and Test**\n",
        "You could use a [randomsplit](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) here, a [Cross Validator](https://spark.apache.org/docs/latest/ml-tuning.html#cross-validation), or another approach of your choice."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmsQfLKA4WwE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvdCBX2D4WNJ"
      },
      "source": [
        "## **Feature Engineering**\n",
        "During this step, I'd like to see you convert at least one STRING variable (such as gender, membership, education or another variable of your choice) into a numeric representation so that you can use it as one of the model inputs. You can convert the string to a numeric by using [one-hot encoding](https://spark.apache.org/docs/latest/ml-features.html#onehotencoderestimator), a [stringindexer](https://spark.apache.org/docs/latest/ml-features.html#stringindexer), etc\n",
        "\n",
        "You will also want to define a ML model object. An example of this would be a random forest, gradient boosting, or some other approach listed [here](https://spark.apache.org/docs/latest/ml-classification-regression.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqAxflMW5IpU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwmmMYzP5MpE"
      },
      "source": [
        "## **Fit/Train ML Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1igoWRu5Pz7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-TVO6s55h05"
      },
      "source": [
        "## **Make Predictions**\n",
        "Use your model to make predications against the Test (holdout) Dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVuqPtEI5hnX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHkwh8mM5Si7"
      },
      "source": [
        "## **Evaluate Model against Test Dataframe**\n",
        "Display model fit statistics, such as RMSE or MSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORJ8UOuH5wPX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}